# Server Benchmark Configuration

# Global benchmark settings
benchmark:
  local_results_dir: "results"           # Local directory to store results

# Benchmark parameters (required)
benchmark_params:
  max_concurrency: 400       # Maximum concurrent requests
  num_prompts: 1200          # Number of prompts to benchmark
  random_input_len: 1000     # Random input length for benchmark
  random_output_len: 1000    # Random output length for benchmark

# Define servers with their models to benchmark
servers:
  - name: "rtx4090_x_1"
    address: "riftuser@142.214.185.238"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"  # 4-bit quantized, similar to Q4_K_M
        tensor_parallel_size: 1
        extra_args: "--max-model-len 8192"

  - name: "rtx4090_x_2"
    address: "riftuser@142.214.185.150"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1  # Each instance uses 1 GPU
        num_instances: 2         # Run 2 instances for 2x throughput
        extra_args: "--max-model-len 8192"
      - name: "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
        tensor_parallel_size: 2  # 72B model needs multi-GPU
        extra_args: "--max-model-len 2048 --kv-cache-dtype fp8"  # Reduced ctx

  - name: "rtx4090_x_4"
    address: "riftuser@142.214.185.230"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1  # Each instance uses 1 GPU
        num_instances: 4         # Run 4 instances for 4x throughput
        extra_args: "--max-model-len 8192"
      - name: "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
        tensor_parallel_size: 2  # 72B model uses 2 GPUs per instance
        num_instances: 2         # Run 2 instances (GPU 0-1, GPU 2-3)
        extra_args: "--max-model-len 2048 --kv-cache-dtype fp8"
      - name: "cpatonn/GLM-4.5-Air-AWQ-4bit"
        tensor_parallel_size: 4  # 106B model uses 4 GPUs
        extra_args: "--dtype float16 --tool-call-parser glm45 --reasoning-parser glm45 --max-model-len 8192 --enable-expert-parallel"

  - name: "rtx5090_x_1"
    address: "riftuser@217.138.104.159"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        extra_args: "--max-model-len 8192"

  - name: "rtx5090_x_2"
    address: "riftuser@217.138.104.153"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        num_instances: 2
        extra_args: "--max-model-len 8192"
      - name: "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
        tensor_parallel_size: 2
        extra_args: "--max-model-len 2048 --kv-cache-dtype fp8"  # Reduced ctx

  - name: "rtx5090_x_4"
    address: "riftuser@217.138.104.158"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        num_instances: 4
        extra_args: "--max-model-len 8192"
      - name: "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
        tensor_parallel_size: 2
        num_instances: 2
        extra_args: "--max-model-len 2048 --kv-cache-dtype fp8"  # Reduced ctx
      - name: "cpatonn/GLM-4.5-Air-AWQ-4bit"
        tensor_parallel_size: 4
        extra_args: "--dtype float16 --tool-call-parser glm45 --reasoning-parser glm45 --max-model-len 8192 --enable-expert-parallel"

  - name: "pro6000_x_1"
    address: "riftuser@74.81.65.29"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        extra_args: "--max-model-len 8192"
      - name: "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
        tensor_parallel_size: 1
        extra_args: "--max-model-len 2048 --kv-cache-dtype fp8"  # Reduced ctx
      - name: "cpatonn/GLM-4.5-Air-AWQ-4bit"
        tensor_parallel_size: 1
        extra_args: "--dtype float16 --tool-call-parser glm45 --reasoning-parser glm45 --max-model-len 8192 --enable-expert-parallel"


# Results will be saved with flat structure using prefixes:
#   {local_results_dir}/{server_name}_{model_name}_system_info.txt
#   {local_results_dir}/{server_name}_{model_name}_hf_download.txt
#   {local_results_dir}/{server_name}_{model_name}_vllm_benchmark.txt
#   {local_results_dir}/{server_name}_{model_name}_yabs.txt
