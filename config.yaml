# Server Benchmark Configuration

# Global benchmark settings
benchmark:
  local_results_dir: "results"           # Local directory to store results

# Optional: Override benchmark parameters
# benchmark_params:
#   max_concurrency: 200
#   num_prompts: 1000
#   random_input_len: 1000
#   random_output_len: 1000

# Define servers with their models to benchmark
servers:
  - name: "rtx4090_x_1"
    address: "riftuser@142.214.185.238"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"  # 4-bit quantized, similar to Q4_K_M
        tensor_parallel_size: 1
        extra_args: "--max-model-len 8192"

  - name: "rtx4090_x_2"
    address: "riftuser@142.214.185.150"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1  # Each instance uses 1 GPU
        num_instances: 2         # Run 2 instances for 2x throughput
        extra_args: "--max-model-len 8192"
      - name: "kosbu/Llama-3.3-70B-Instruct-AWQ"
        tensor_parallel_size: 2  # 72B model needs multi-GPU
        extra_args: "--max-model-len 4096"  # Reduced ctx

  - name: "rtx4090_x_4"
    address: "riftuser@142.214.185.230"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1  # Each instance uses 1 GPU
        num_instances: 4         # Run 4 instances for 4x throughput
        extra_args: "--max-model-len 8192"
      - name: "kosbu/Llama-3.3-70B-Instruct-AWQ"
        tensor_parallel_size: 2  # 72B model uses 2 GPUs per instance
        num_instances: 2         # Run 2 instances (GPU 0-1, GPU 2-3)
        extra_args: "--max-model-len 4096"
      - name: "cpatonn/GLM-4.5-Air-AWQ-4bit"
        tensor_parallel_size: 4  # 106B model uses 4 GPUs
        extra_args: "--dtype float16 --tool-call-parser glm45 --reasoning-parser glm45 --max-model-len 8192"

  - name: "rtx5090_x_1"
    address: "riftuser@217.138.104.159"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        extra_args: "--max-model-len 8192"

  - name: "rtx5090_x_2"
    address: "riftuser@217.138.104.153"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        num_instances: 2
        extra_args: "--max-model-len 8192"
      - name: "kosbu/Llama-3.3-70B-Instruct-AWQ"
        tensor_parallel_size: 2
        extra_args: "--max-model-len 4096"  # Reduced ctx

  - name: "rtx5090_x_4"
    address: "riftuser@217.138.104.158"
    ssh_key: "~/.ssh/id_ed25519"
    port: 22
    models:
      - name: "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
        tensor_parallel_size: 1
        num_instances: 4
        extra_args: "--max-model-len 8192"
      - name: "kosbu/Llama-3.3-70B-Instruct-AWQ"
        tensor_parallel_size: 2
        num_instances: 2
        extra_args: "--max-model-len 4096"  # Reduced ctx
      - name: "cpatonn/GLM-4.5-Air-AWQ-4bit"
        tensor_parallel_size: 4
        extra_args: "--dtype float16 --tool-call-parser glm45 --reasoning-parser glm45 --max-model-len 8192"


# Results will be saved with flat structure using prefixes:
#   {local_results_dir}/{server_name}_{model_name}_system_info.txt
#   {local_results_dir}/{server_name}_{model_name}_hf_download.txt
#   {local_results_dir}/{server_name}_{model_name}_vllm_benchmark.txt
#   {local_results_dir}/{server_name}_{model_name}_yabs.txt
